# This script is based on MCGaze/MCGaze_demo/demo.ipynb of https://github.com/zgchen33/mcgaze.
# All English comments were added by me (Frank Schilling).
# Note: If there is more than one human head detected in a video frame then the estimated gaze
# for that frame in Output/processed_video.csv will be nan!

import cv2
from facenet_pytorch import MTCNN
import os

from pathlib import Path

from mmdet.apis import init_detector
from mmdet.datasets.pipelines import Compose
import torch
from mmcv.parallel import collate, scatter
import numpy as np

import math
import argparse



def parse_args():

    parser = argparse.ArgumentParser(description='Estimate gaze of images generated by head_det.py')
    
    parser.add_argument(
        '-v',
        help='visualize output (creates a new video with estimated gaze)',
        action='store_true'
        )
    
    return parser.parse_args()


# Returns information about the video that was preprocessed by head_det.py:
# 1. the path of the video (otherwise this script wouldn't know which video it analyzes,
#    because it just works with the images of each video frame that were generated by head_det.py)
# 2. the FPS of the video
def get_source_video_info():
    with open('result/processed_video.txt', 'r') as f:
        # Discard first line as it is just a description of the contents.
        f.readline()
        source_video_path = f.readline()
        fps = float(f.readline())
        
        return source_video_path, fps


# File will be written to
# CWD/Output/filename_of_video_without_file_extension.csv
# where filename_of_video_without_file_extension is a parameter of this function.
def write_estimated_gaze_to_file(filename_of_video_without_file_extension, video_clip_list, video_fps):

    output_path = str(Path.cwd()) + '/Output/' + filename_of_video_without_file_extension + '.csv'
    
    with open(output_path, 'w') as f:
        
        f.write('frame,timestamp in s,success,x of gaze vector,y of gaze vector,z of gaze vector,yaw in radians,pitch in radians\n')
        
        for video_clip in video_clip_list:
            for i, current_frame in enumerate(video_clip['frame_id']):

                is_exactly_one_person_in_frame = ('gaze_p0' in video_clip) and ('gaze_p1' not in video_clip)
                
                gaze_vector_magnitude = math.sqrt(
                    math.pow(video_clip['gaze_p0'][i][0][0], 2) +
                    math.pow(video_clip['gaze_p0'][i][0][1], 2) +
                    math.pow(video_clip['gaze_p0'][i][0][2], 2)
                    )
                
                normalized_gaze_vector = [
                    video_clip['gaze_p0'][i][0][0] / gaze_vector_magnitude,
                    video_clip['gaze_p0'][i][0][1] / gaze_vector_magnitude,
                    video_clip['gaze_p0'][i][0][2] / gaze_vector_magnitude
                ]

                f.write('{},{},{},{},{},{},{},{}\n'.format(
                    current_frame+1,
                    round(float(current_frame) * (1.0 / video_fps), 3), # +/- 0.001 radians (less 0.1 degrees) can be rounded off (easier to compare output file to output from OpenFace)
                    1 if is_exactly_one_person_in_frame else 0,
                    # Write nan to file if there is more than one human head found in the current frame. In this case I don't know whose gaze to estimate.
                    -video_clip['gaze_p0'][i][0][0] if is_exactly_one_person_in_frame else math.nan, # adjust to OpenFace format by negating it
                    -video_clip['gaze_p0'][i][0][1] if is_exactly_one_person_in_frame else math.nan, # adjust to OpenFace format by negating it
                    video_clip['gaze_p0'][i][0][2] if is_exactly_one_person_in_frame else math.nan,
                    math.atan2(normalized_gaze_vector[0], normalized_gaze_vector[2]),
                    math.asin(normalized_gaze_vector[1])
                    ))


def load_datas(data, test_pipeline, datas):
    datas.append(test_pipeline(data))


def infer(datas,model,clip,i):
    datas = sorted(datas, key=lambda x:x['img_metas'].data['filename']) # 按帧顺序 img名称从小到大
    datas = collate(datas, samples_per_gpu=len(frame_id)) # 用来形成batch用的
    datas['img_metas'] = datas['img_metas'].data
    datas['img'] = datas['img'].data
    datas = scatter(datas, ["cuda:0"])[0]
    with torch.no_grad():
        (det_bboxes, det_labels), det_gazes = model(
                return_loss=False,
                rescale=True,
                format=False,# 返回的bbox既包含face_bboxes也包含head_bboxes
                **datas)    # 返回的bbox格式是[x1,y1,x2,y2],根据return_loss函数来判断是forward_train还是forward_test.
    gaze_dim = det_gazes['gaze_score'].size(1)
    det_fusion_gaze = det_gazes['gaze_score'].view((det_gazes['gaze_score'].shape[0], 1, gaze_dim))
    clip['gaze_p'+str(i)].append(det_fusion_gaze.cpu().numpy())



if __name__ == '__main__':

    args = parse_args()

    frame_id = 0
    person_num = 0

    # Holds information about a sequence of frames in which the amount of people stays the same.
    # Example:
    # frame i contains 2 people,
    # frame i+1 contains 3 people,
    # frame i+2 also contains 3 people and
    # frame i+3 contains 4 people. Then the frames (i+1) and (i+2) form a video_clip that looks like this:
    # {
    #   # following keys and values are added in the next loop
    #   'frame_id': [i+1, i+2],
    #   person_num: 3,
    #   'p0': [ position_of_head0_in_framei+1, position_of_head0_in_framei+2 ],
    #   'p1': [ position_of_head1_in_framei+1, position_of_head1_in_framei+2 ],
    #   'p2': [ position_of_head2_in_framei+1, position_of_head2_in_framei+2 ],
    #
    #    # following keys and values are added in the loop that follows after the next loop
    #    'gaze_p0': [ gaze_vector_of_head0_in_framei+1, gaze_vector_of_head0_in_framei+2 ],
    #    'gaze_p1': [ gaze_vector_of_head1_in_framei+1, gaze_vector_of_head1_in_framei+2 ],
    #    'gaze_p2': [ gaze_vector_of_head2_in_framei+1, gaze_vector_of_head2_in_framei+2 ]
    # }
    video_clip=None
    
    video_clip_set = []
    vid_len = len(os.listdir(str(Path.cwd()) + '/frames'))
    
    while frame_id < vid_len:
        frame = cv2.imread(str(Path.cwd()) + ('/frames/%d.jpg' % frame_id))
        w,h,c = frame.shape
        txt_path = str(Path.cwd()) + ('/result/labels/%d.txt' % frame_id)
        f = open(txt_path, 'r')
        # list of bounding boxes [x1, y1, x2, y2] for each head that was detected in the current frame
        face_bbox = []
        for line in f.readlines():
            line = line.strip()
            line = line.split(' ')
            for i in range(len(line)):
                line[i] = eval(line[i])
                #将每一行的数据存入字典
            if line[0]==1:
                face_bbox.append([(line[1]),(line[2]),(line[3]),(line[4])])
        f.close()
        #按第一维排序
        if face_bbox is not None:
            face_bbox = sorted(face_bbox, key= lambda x :x[0])
            # the amount of people in the current frame equals the amount of heads found
            cur_person_num = len(face_bbox)
        else:
            cur_person_num = 0
        if cur_person_num != person_num :
            if video_clip==None:
                video_clip={'frame_id': [], 'person_num': cur_person_num}
                video_clip['frame_id'].append(frame_id)
                # assign to video_clip['p0'] the location of head 0 in the current frame,
                # assign to video_clip['p1'] the locations of head 1 in the current frame,
                # etc.
                for i in range(cur_person_num):
                    video_clip['p'+str(i)]=[face_bbox[i]]
            else:
                video_clip_set.append(video_clip)

                video_clip={'frame_id': [], 'person_num': cur_person_num}
                video_clip['frame_id'].append(frame_id)
                for i in range(cur_person_num):
                    video_clip['p'+str(i)]=[face_bbox[i]]
        else:
            #
            # possible BUG:
            # If there is no head to be found in the first frame then
            # video_clip==None and cur_person_num==person_num==0, hence this block
            # will be executed, trying to append to video_clip, but video_clip==None.
            #

            video_clip['frame_id'].append(frame_id)
            for i in range(cur_person_num):
                    video_clip['p'+str(i)].append(face_bbox[i])
        person_num = cur_person_num
        frame_id += 1

    video_clip_set.append(video_clip)




    model = init_detector(
            os.path.dirname(str(Path.cwd())) + '/configs/multiclue_gaze/multiclue_gaze_r50_gaze360.py',
            os.path.dirname(str(Path.cwd())) + '/ckpts/multiclue_gaze_r50_gaze360.pth',
            device="cuda:0",
            cfg_options=None,)
    cfg = model.cfg




    #print(cfg.data.test.pipeline[1:])
    test_pipeline = Compose(cfg.data.test.pipeline[1:])




    max_len = 100

    for clip in video_clip_set:
        frame_id = clip['frame_id']
        person_num = clip['person_num']
        for i in range(person_num):
            # contains positions of head i for each frame of the current video clip
            head_bboxes = clip['p'+str(i)]
            clip['gaze_p'+str(i)] = []
            datas = []
            for j,frame in enumerate(frame_id):
                cur_img = cv2.imread(str(Path.cwd()) + "/frames/"+str(frame)+".jpg")
                w,h,_ = cur_img.shape
                for xy in head_bboxes[j]:
                    # position of head i in the current frame
                    xy = int(xy)
                head_center = [int(head_bboxes[j][1]+head_bboxes[j][3])//2,int(head_bboxes[j][0]+head_bboxes[j][2])//2]
                l = int(max(head_bboxes[j][3]-head_bboxes[j][1],head_bboxes[j][2]-head_bboxes[j][0])*0.8)
                head_crop = cur_img[max(0,head_center[0]-l):min(head_center[0]+l,w),max(0,head_center[1]-l):min(head_center[1]+l,h),:]
                w_n,h_n,_ = head_crop.shape
                # if frame==0:
                #     plt.imshow(head_crop)
                # print(head_crop.shape)
                cur_data = dict(filename=j,ori_filename=111,img=head_crop,img_shape=(w_n,h_n,3),ori_shape=(2*l,2*l,3),img_fields=['img'])
                load_datas(cur_data,test_pipeline,datas)
                
                if len(datas)>max_len or j==(len(frame_id)-1):
                    infer(datas,model,clip,i)
                    datas = []
                    if j==(len(frame_id)-1):
                        clip['gaze_p'+str(i)] = np.concatenate(clip['gaze_p'+str(i)],axis=0)




    for vid_clip in video_clip_set:
        for i,frame_id in enumerate(vid_clip['frame_id']):  # 遍历每一帧
            cur_img = cv2.imread(str(Path.cwd()) + "/frames/"+str(vid_clip['frame_id'][i])+".jpg")
            for j in range(vid_clip['person_num']):  # 遍历每一个人
                gaze = vid_clip['gaze_p'+str(j)][i][0]
                head_bboxes = vid_clip['p'+str(j)][i]
                for xy in head_bboxes:
                    xy = int(xy)
                head_center = [int(head_bboxes[1]+head_bboxes[3])//2,int(head_bboxes[0]+head_bboxes[2])//2]
                l = int(max(head_bboxes[3]-head_bboxes[1],head_bboxes[2]-head_bboxes[0])*1)
                gaze_len = l*1.0
                thick = max(5,int(l*0.01))
                cv2.arrowedLine(cur_img,(head_center[1],head_center[0]),
                            (int(head_center[1]-gaze_len*gaze[0]),int(head_center[0]-gaze_len*gaze[1])),
                            (230,253,11),thickness=thick)
            cv2.imwrite(str(Path.cwd()) + '/new_frames/%d.jpg' % frame_id, cur_img)




    img = cv2.imread(str(Path.cwd()) + '/new_frames/0.jpg')  #读取第一张图片
    source_video_path, video_fps = get_source_video_info()
    imgInfo = img.shape
    size = (imgInfo[1],imgInfo[0])  #获取图片宽高度信息
    print('image (width, height) =', size)

    write_estimated_gaze_to_file(Path(source_video_path).stem, video_clip_set, video_fps)

    if args.v:
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        videoWrite = cv2.VideoWriter(str(Path.cwd()) + '/Output/' + Path(source_video_path).stem + '.mp4',fourcc,video_fps,size)
        files = os.listdir(str(Path.cwd()) + '/new_frames/')
        out_num = len(files)
        for i in range(0,out_num):
            fileName = str(Path.cwd()) + '/new_frames/'+str(i)+'.jpg'    #循环读取所有的图片,假设以数字顺序命名
            img = cv2.imread(fileName)
        
            videoWrite.write(img)# 将图片写入所创建的视频对象

        videoWrite.release()